{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5qIvBkNBtXz",
        "outputId": "99d63bbb-bebc-4647-bec2-2d8e056f7e2b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cVs5l7oCRlG"
      },
      "source": [
        "import os\n",
        "\n",
        "AI6Hz_normal, AI6Hz_anomalies = [], []\n",
        "root_folder_norm = 'drive/MyDrive/data/data_Marcel/AI6Hz_normal/'\n",
        "for root, dirs, files in os.walk(root_folder_norm, topdown=False):\n",
        "    for name in files:\n",
        "      if name.endswith('.txt'):\n",
        "        AI6Hz_normal.append(name)\n",
        "root_folder_anom = 'drive/MyDrive/data/data_Marcel/AI6Hz_anomalies/'\n",
        "for root, dirs, files in os.walk(root_folder_anom, topdown=False):\n",
        "    for name in files:\n",
        "      if name.endswith('.txt'):\n",
        "        AI6Hz_anomalies.append(name)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFc3PBNtCug2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from os.path import dirname, join as pjoin\n",
        "import scipy.io as sio\n",
        "import scipy\n",
        "def main2(s_train, s_evaluate, ts = None, batch_size = 256):\n",
        "  from sklearn.preprocessing import   MinMaxScaler\n",
        "  import datetime\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import LSTM\n",
        "  from tensorflow.keras.layers import Dense\n",
        "  from tensorflow.keras.layers import RepeatVector\n",
        "  from tensorflow.keras.layers import TimeDistributed\n",
        "  from tensorflow.keras.utils import plot_model\n",
        "  from sklearn import preprocessing\n",
        "  from keras import backend as K \n",
        "  import random\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  s_train = scaler.fit_transform(s_train.reshape(-1, 1))\n",
        "  s_evaluate = scaler.fit_transform(s_evaluate.reshape(-1, 1))\n",
        "  seed_value= 1\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "  random.seed(seed_value)\n",
        "  np.random.seed(seed_value)\n",
        "  tf.random.set_seed(seed_value)\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads = 1, inter_op_parallelism_threads = 1)\n",
        "  sess = tf.compat.v1.Session(graph =  tf.compat.v1.get_default_graph(), config = session_conf)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "  start_time = datetime.datetime.now()\n",
        "  df_daily_jumpsup = s_evaluate\n",
        "  df_small_noise = s_train #s_train[0:len(df_daily_jumpsup)] \n",
        "  training_mean = np.mean(df_small_noise)\n",
        "  training_std = np.std(df_small_noise)\n",
        "  #scaler = StandardScaler()\n",
        "  #df_training_value = scaler.fit_transform(df_small_noise.reshape(-1, 1))\n",
        "  df_training_value = df_small_noise.reshape(-1, 1)\n",
        "  print(\"Number of training samples:\", len(df_training_value))\n",
        "  TIME_STEPS =  np.floor(np.std(signal_test) * 5387).astype('int64') if ts == None else ts\n",
        "  # Generated training sequences for use in the model.\n",
        "  def create_sequences(values, time_steps = TIME_STEPS):\n",
        "      output = []\n",
        "      for i in range(TIME_STEPS, len(values)- TIME_STEPS):\n",
        "          output.append(values[i : (i + time_steps)])\n",
        "      return np.stack(output)\n",
        "  x_train = create_sequences(df_training_value)\n",
        "  x_train = x_train.reshape(x_train.shape[0], TIME_STEPS, 1)\n",
        "  print(x_train.shape)\n",
        "  log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, activation = 'tanh', recurrent_dropout = 0, input_shape = (x_train.shape[1],x_train.shape[2])\n",
        "  ,return_sequences = True))\n",
        "  model.add(LSTM(64, activation = 'tanh', recurrent_dropout = 0, return_sequences = False))\n",
        "  model.add(keras.layers.Dropout(rate = 0.2))\n",
        "  model.add(RepeatVector(x_train.shape[1]))\n",
        "  model.add(keras.layers.Dropout(rate = 0.2))\n",
        "  model.add(LSTM(64, activation = 'tanh', recurrent_dropout = 0, return_sequences = True))\n",
        "  model.add(LSTM(100, activation = 'tanh', recurrent_dropout = 0, return_sequences = True))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.001), loss = \"mse\")\n",
        "  model.summary()\n",
        "  history = model.fit(\n",
        "      x_train,\n",
        "      x_train,\n",
        "      epochs = 100,\n",
        "      batch_size = batch_size,\n",
        "      validation_split = 0.25,\n",
        "      callbacks = [\n",
        "          keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 15, mode = \"min\")\n",
        "      ,[tensorboard_callback]],\n",
        "      verbose = 0\n",
        "      )\n",
        "  x_train_pred = model.predict(x_train)\n",
        "  train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis = 1)\n",
        "  threshold = np.abs(np.percentile(train_mae_loss,95)+np.percentile(train_mae_loss,5))\n",
        "  print(\"Reconstruction error threshold: \", threshold)\n",
        "  #df_test_value = scaler.fit_transform(df_daily_jumpsup.reshape(-1, 1))\n",
        "  df_test_value = df_daily_jumpsup.reshape(-1, 1)\n",
        "  x_test = create_sequences(df_test_value)\n",
        "  x_test = x_test.reshape(x_test.shape[0], TIME_STEPS, 1)\n",
        "  print(\"Test input shape: \", x_test.shape)\n",
        "  # Get test MAE loss.\n",
        "  x_test_pred = model.predict(x_test)\n",
        "  test_mae_loss =np.mean(np.abs(x_test_pred - x_test), axis = 1)\n",
        "  test_mae_loss = test_mae_loss.reshape((-1))\n",
        "  anomalies = test_mae_loss > threshold\n",
        "  anomalous_data_indices = []\n",
        "  for data_idx in range(TIME_STEPS - 1, len(x_test) - TIME_STEPS + 1):\n",
        "      if sum(anomalies[data_idx - TIME_STEPS + 1 : data_idx]) >= TIME_STEPS / 2:\n",
        "          anomalous_data_indices.append(data_idx)\n",
        "  anomalies[data_idx - TIME_STEPS + 1 : data_idx]\n",
        "  df1 = s_evaluate\n",
        "  df_subset = df1[anomalous_data_indices]\n",
        "  end_time =  datetime.datetime.now()\n",
        "  exec_time = end_time - start_time\n",
        "  print(f'execution time {exec_time}')\n",
        "  return x_train, x_test, x_test_pred, train_mae_loss, test_mae_loss, anomalous_data_indices\n",
        "\n",
        "def plot2_1(df_training_value, df_test_value, x_test_pred, train_mae_loss, test_mae_loss, anomalous_data_indices, title = None):\n",
        "  import plotly.graph_objects as go\n",
        "  from plotly.subplots import make_subplots\n",
        "  import plotly\n",
        "  from sklearn import preprocessing\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  fig = make_subplots(rows = 5, cols = 2, horizontal_spacing = 0.1, vertical_spacing = 0.03,\n",
        "  specs=[[{}, {\"rowspan\": 2}],\n",
        "          [{}, None],\n",
        "          [{},None],\n",
        "          [{}, {\"rowspan\": 2}],\n",
        "          [{},None]],\n",
        "          print_grid = True,\n",
        "          shared_xaxes= True,\n",
        "          column_widths = (0.7, 0.3))\n",
        "  fig.add_trace(go.Scatter( y = df_training_value,\n",
        "                          line_shape = 'linear', name = 'Train', marker_color = '#000000'), row = 1, col = 1)\n",
        "  \n",
        "  fig.add_trace(go.Histogram(x = train_mae_loss.flatten(),\n",
        "                            nbinsx = 30, marker_color = '#58D68D', name = 'train mae loss', opacity = 0.5), row = 1, col = 2)\n",
        "  \n",
        "  fig.add_trace(go.Histogram(x = test_mae_loss.flatten(),\n",
        "                            nbinsx = 30, marker_color = '#D35400', name = 'test mae loss', opacity = 0.6), row = 4, col = 2)\n",
        "    \n",
        "  fig.add_trace(go.Scatter(y = df_test_value,\n",
        "                          line_shape = 'linear', name = 'Test', line = dict(width = 2, color = '#000000')), row = 2, col = 1)\n",
        "\n",
        "  fig.add_trace(go.Scatter(y = x_test_pred,\n",
        "                          line_shape = 'linear', name = 'RNN pred', line = dict(width = 2, color = '#000000')), row = 3, col = 1)\n",
        "  \n",
        "  fig.add_trace(go.Scatter(y = test_mae_loss,\n",
        "                          line_shape = 'linear', name = 'Error', line = dict(width = 2, color = '#990000')), row = 4, col = 1)\n",
        "  \n",
        "  fig.add_trace(go.Scatter( y = df_test_value,\n",
        "                          line_shape = 'linear', name = 'Signal', marker_color = '#000000'), row = 5, col = 1)\n",
        "  B = list(anomalous_data_indices)\n",
        "  A = []\n",
        "  C = []\n",
        "  #next index\n",
        "  windows_size = 2\n",
        "  N_index = windows_size\n",
        "  #actual index\n",
        "  A_index = 0\n",
        "  for i in range(len(B) - windows_size):\n",
        "    if (B[N_index] - B[A_index]) / B[N_index] < 0.02:\n",
        "      C.append(B[A_index])\n",
        "      N_index += 1\n",
        "      A_index += 1\n",
        "    else:\n",
        "      C.append(B[A_index])\n",
        "      A.append(C)\n",
        "      N_index = N_index + 1\n",
        "      A_index = A_index + 1\n",
        "      C = []\n",
        "      # C.append(B[N_index])\n",
        "\n",
        "  flat_list = [item for sublist in A for item in sublist]\n",
        "  A.append([item for item in B if item not in flat_list])\n",
        "  condition = 0\n",
        "  for sublist in A:\n",
        "    fig.add_trace(go.Scatter(x = sublist, y = df_test_value[sublist],\n",
        "                            name = 'Anomaly', showlegend = True if condition < 1 == True else False , mode='markers',\n",
        "                            marker=dict(size=3,\n",
        "                            color='#990000'), opacity = 0.8),\n",
        "                            row = 5, col = 1)\n",
        "    condition +=1\n",
        "  fig.add_trace(go.Scatter(x = np.linspace(0,len(test_mae_loss),20000), y = np.linspace(np.max(train_mae_loss),np.max(train_mae_loss),20000),\n",
        "                            line_shape = 'linear', name = 'Threshold',\n",
        "                            line = dict(width = 2, color = '#138D75'),opacity = 0.7,\n",
        "                            hovertemplate = f'Threshold: {np.max(train_mae_loss)}'\n",
        "                            ), row = 4, col = 1)\n",
        "  \n",
        "  fig.update_layout(shapes = [\n",
        "      dict(\n",
        "        type = 'line',\n",
        "        yref = 'paper', y0 = 0, y1 = 0.4 - 0.015,\n",
        "        xref = 'x6', x0 = np.max(train_mae_loss), x1 = np.max(train_mae_loss),\n",
        "        line = dict(color = '#990000', width = 1.5)\n",
        "              )     \n",
        "  ],\n",
        "  plot_bgcolor='rgb(255, 255, 255)',\n",
        "  #height = 600, width = 'device-width',\n",
        "  title = title\n",
        "  )\n",
        "\n",
        "  fig.update_xaxes(showgrid = True, gridwidth = 1, gridcolor = 'Gray',\n",
        "                  zeroline = True, zerolinecolor = '#000',row = 1, col = 2,\n",
        "                  nticks = 5\n",
        "                  )\n",
        "  fig.update_xaxes(showgrid = True, gridwidth = 1, gridcolor = 'Gray',\n",
        "                  zeroline = True, zerolinecolor = '#000',row = 4, col = 2,\n",
        "                  nticks = 5\n",
        "                  )\n",
        "\n",
        "  fig.update_yaxes(showgrid = True, gridwidth = 1, gridcolor = 'Gray',\n",
        "                  zeroline = False, zerolinewidth = 2, zerolinecolor = '#000', \n",
        "                  row = 1, col = 2, nticks = 5\n",
        "                  )\n",
        "  fig.update_yaxes(showgrid = True, gridwidth = 1, gridcolor = 'Gray',\n",
        "                  zeroline = False, zerolinewidth = 2, zerolinecolor = '#000', \n",
        "                  row = 4, col = 2, nticks = 5\n",
        "                  )\n",
        "\n",
        "  fig.add_annotation(\n",
        "        x = np.abs(np.percentile(train_mae_loss,95)+np.percentile(train_mae_loss,5)),\n",
        "        y = -0.7,\n",
        "        xref=\"x2\",\n",
        "        yref=\"paper\",\n",
        "        text= '{0:.2f}'.format(np.max(train_mae_loss)),\n",
        "        showarrow=False,\n",
        "        font=dict(\n",
        "            family=\"Courier New, monospace\",\n",
        "            size=11,\n",
        "            color=\"#ffffff\"\n",
        "            ),\n",
        "        align=\"center\",\n",
        "        bordercolor=\"#990000\",\n",
        "        borderwidth=1,\n",
        "        borderpad=2,\n",
        "        bgcolor=\"#990000\",\n",
        "        opacity=1\n",
        "        , row = 1, col = 2\n",
        "        )\n",
        "  # fig.add_annotation(\n",
        "  #       x = -0.5,\n",
        "  #       y = np.max(train_mae_loss)*2,\n",
        "  #       xref= \"paper\",\n",
        "  #       yref=\"y\",\n",
        "  #       text= 'Threshold: {0:.2f}'.format(np.max(train_mae_loss)),\n",
        "  #       showarrow=False,\n",
        "  #       font=dict(\n",
        "  #           family=\"Courier New, monospace\",\n",
        "  #           size=11,\n",
        "  #           color=\"#ffffff\"\n",
        "  #           ),\n",
        "  #       align=\"center\",\n",
        "  #       bordercolor=\"#138D75\",\n",
        "  #       borderwidth=1,\n",
        "  #       borderpad=2,\n",
        "  #       bgcolor=\"#138D75\",\n",
        "  #       opacity=1\n",
        "  #       , row = 3, col = 2\n",
        "  #       )\n",
        "  fig.add_annotation(\n",
        "        x = np.max(train_mae_loss),\n",
        "        y = 1,\n",
        "        xref=\"x6\",\n",
        "        yref=\"paper\",\n",
        "        text= '{0:.2f}'.format(np.max(train_mae_loss)),\n",
        "        showarrow=False,\n",
        "        font=dict(\n",
        "            family=\"Courier New, monospace\",\n",
        "            size=11,\n",
        "            color=\"#ffffff\"\n",
        "            ),\n",
        "        align=\"center\",\n",
        "        bordercolor=\"#990000\",\n",
        "        borderwidth=1,\n",
        "        borderpad=2,\n",
        "        bgcolor=\"#990000\",\n",
        "        opacity=1\n",
        "        , row = 4, col = 2\n",
        "        )\n",
        "\n",
        "  return fig"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpXpj4lBDA5E"
      },
      "source": [
        "import plotly\n",
        "import datetime\n",
        "ts = 190\n",
        "for signal in AI6Hz_normal:\n",
        "  if signal.replace('n', 'a') in AI6Hz_anomalies:\n",
        "    signal_train = noise_reduct(signal_miliseconds(root_folder_norm, signal))\n",
        "    signal_test  = noise_reduct(signal_miliseconds(root_folder_anom, signal.replace('n', 'a')))\n",
        "    asize= [signal_train.shape if signal_train.shape < signal_test.shape else signal_test.shape][0][0]\n",
        "    signal_train = signal_train[ : asize]\n",
        "    signal_test  = signal_test[  : asize]\n",
        "    df_training_value, df_test_value, x_test_pred, train_mae_loss, test_mae_loss, anomalous_data_indices = main2(signal_train, signal_test, ts = ts)\n",
        "    fig = plot2_1(np.squeeze(df_training_value)[:,0], np.squeeze(df_test_value)[:,0], np.squeeze(x_test_pred)[:,0], train_mae_loss, test_mae_loss, anomalous_data_indices, title = str(signal) + f'  TimeSteps = {ts}')\n",
        "    plotly.offline.plot(fig, filename='drive/MyDrive/htmlplots/'+str(signal.split('.')[0])+f'ts{ts}_{datetime.datetime.now().strftime(\"%a, %d %B %Y %H:%M:%S\")}.html')\n",
        "  print(\"done!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}